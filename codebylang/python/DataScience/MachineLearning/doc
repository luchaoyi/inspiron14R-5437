knn:
    knn可同时用于分类，和数值预测，近邻投票决定分类，近邻加权平均预测数值

决策树:
    度量数据混乱度，依据混乱度降低程度切分数据,使数据趋于稳定。
    混乱度度量:
        熵
        Gini不纯度
        方差
    CART:二元切分数据,形成一个二叉树.离散型分类采用投票表决法;对于连续型CART叶几点有:
        回归树：叶节点存放一个均值常数
        模型树：叶几点存放一个线性拟合模型
        决策树拟合，是对非线性数据进行分段拟合

logstic回归:
    二分类问题，sigmod函数为概率。最大似然估计导出训练方法：梯度上升.
    梯度上升:
        批处理：一次利用整个数据集
        随机梯度上升：随机选择一个样本，遍历数据集。在线学习算法，支持增量学习
集成方法:
    多个分类器集成，可以是不同分类器，也可以是同一分类器：
    bagging：多重随机抽样形成多个数据用于训练不同分类器，集成判断
    boost:
        赋权:
            样本赋权:错分样本权值变大，未错分样本权值变小,更加关注错分样本
                错分样本Di=Di*e^-alpha/sum(D)
                正确分类样本Di=Di*e^alpha/sum(D)
            分类器权值:
                err=错分/所有 (错误率)
                alpha = 1/2 ln(1-err/err)
                步骤:
                    for i --> num:
                        构建分类器
                        更新alpha
                        根据此分类器分类误差更新D,是新分类器吸取教训
                        #分类器只有将D纳入误差考核，并对D大的惩罚更大，才能促使D起作用
                        计算此时集成分类器分类误差
                        if 集成误差==0 :break
                        #else
                        循环在构建一个弱分类器

回归:   
    局部加权线性回归：对于不同样本的误差赋予不同的权值，训练时根据加权误差训练，权值大的样本影响更大。
    lasso:增加 E|wk|<lambda限制权系数增大，lambda很小时一些系数被迫缩减到0,达到了降维的目的
    前向逐步回归:初始权重为1,每一步都某个系数增加或减小一个值


Apriori
	生成频繁项集
		Fk-1*Fk-1方法生成频繁项，即前k-1相同的k项集生成k+1项集
		k项集不频繁不参与生成k+1,因为一定不频繁
		对于生成的K+1项集合，依然要进行计数，判定是否频繁，因为两个频繁项生成的k+1不一定频繁
	挖掘关联规则
		P->H的置信度  support((P,H)/support(P)
		从频繁项集生成关联规则的方法与生成频繁项集类似，剪枝方法也类似
FP
	扫描整个数据库，为单个项建立一张item:count的表，根据item的count，过滤掉数据集所有记录中不满足支持度的item，过滤后将记录按出现次数排序
	扫描过滤重排后的数据集建立FP树,此时FP树压缩存储了整个数据集
	从FP树中挖掘频繁项
		从FP数抽取条件模式基，某个元素的条件模式基是和此元素共现的数据集
		如t的条件基中,满足支持度的单个元素x,和t共现
			t的条件基下的x的条件基和t,x共现




TensorFlow
	为了在Python中进行高效的数值计算，我们通常会使用像NumPy一类的库，将一些诸如矩阵乘法的耗时操作在Python环境的外部来计算，这些计算通常会通过其它语言并用更为高效的代码来实现。但遗憾的是，每一个操作切换回Python环境时仍需要不小的开销。如果你想在GPU或者分布式环境中计算时，这一开销更加可怖，这一开销主要可能是用来进行数据迁移。TensorFlow也是在Python外部完成其主要工作，但是进行了改进以避免这种开销。其并没有采用在Python外部独立运行某个耗时操作的方式，而是先让我们描述一个交互操作图，然后完全将其运行在Python外部。这与Theano或Torch的做法类似。因此Python代码的目的是用来构建这个可以在外部运行的计算图，以及安排计算图的哪一部分应该被运行
	在Session中提交计算图，在python外根据计算图执行计算
	TensorFlow计算图没有递归的有向图，这允许计算并行
	
	
/×
	TensorFlow的计算图由Operation对象和Tensor对象构成，对象可以通过id or name得到。#html DOM对象思想
	 
×/
	
TensorFlow算法的一般流程：
	导入或生成数据集
	变换和归一化数据
	划分数据集，为测试，训练，验证
	初始化算法参数（超参数）
		定义常量，变量，和占位符
			变量定义时给初始化值，使用前需要显式定义初始化操作，并在Session中显式执行初始化操作。
			占位符，使用前没有确切值的变量，不需要初始化值。
	定义计算模型
	声明损失函数：定义模型后，我们必须能够进行评估输出
	评估模型
	调整超参数
	部署/预测
	

张量
	可以使用tf内置函数创建张量
	可以使用convert_to_tensor将python常量转换为张量
	
In [20]: tf.convert_to_tensor([1,2,3])
Out[20]: <tf.Tensor 'Const_1:0' shape=(3,) dtype=int32>
In [21]: tf.convert_to_tensor(1)
Out[21]: <tf.Tensor 'Const_2:0' shape=() dtype=int32>


#变量一般存储模型需要训练的参数，占位符一般为外部数据位置

变量
	变量使用前必须初始化
	my_var = tf.Variable(tf.zeros([2,3])) #声明变量
	sess = tf.Session()
	initialize_op = tf.global_variables_initializer () #初始化方法
	sess.run(initialize_op)#提交计算，执行初始化操作
	
	global_variables_initializer 方法初始化所有变量，如果变量初始化具有依赖性，则调用变量自身初始化器按顺序初始化
	
	sess = tf.Session()
	first_var = tf.Variable(tf.zeros([2,3]))
	sess.run(first_var.initializer)
	second_var = tf.Variable(tf.zeros_like(first_var)) 
	sess.run(second_var.initializer)	# Depends on first_var
	
占位符	
	just holding the position for data,get data from a feed_dict argument in the session
	

Activation Functions of neural networks
		tf.nn.relu ->  max(0,x)
		tf.nn.relu6-> min(max(0,x),6) #x>6 o=6,x<0 o=0,0<=x<=6,o=x
		tf.nn.sigmoid->1/(1+exp(-x))
		tf.nn.tanh->exp(-x))/(exp(x)+exp(-x))
		tf.nn.softsign->x/(abs(x) + 1)
		tf.nn.softplus->log(exp(x) + 1)
		tf.nn.elu->  if x < 0 (exp(x)+1) else x		


--------------------------------
cholesky分解线性回归
	A=LL^T，A是对称的，正定的矩阵，A为实对称正定矩阵时，它相当于LU三角分解法的变形。
	TensorFlow函数cholesky()只返回分解的下部对角线部分	
	LL.Tx=b,令L.Tx=y,Ly=b,先slove Ly=b,在slove L.Tx=y #tf.matrix_solve()求解线性方差组
	线性回归的cholesky分解方法
		Ax=b,A不一定是对称正定的,变形,A.TAx=Ab
		

SVM
	样本线性可分，具有多条线可以分类.间隔定义为2/||A||,寻找可线性可分线中间隔最大的.
	max 2/||A|| -> min ||A|| ,即选择模型复杂度最低的->min ||A||;subject to yi(Axi-b)>=1 任意i;
	由目标得到损失函数
		1/n×SUM[i=1:n]max(0,1-yi(Axi-b))  + alpha||A||^2
		若1-yi(Axi-b)>0则惩罚(不满足约束),否则惩罚为0,即不惩罚;||A||越大惩罚越大
		alpha is the soft separability regularization parameter.
		
		
	SVM回归	
		1/n×SUM[i=1:n]max(0,|yi-(Axi-b)|-E)  + alpha||A||^2
		|yi-(Axi+b)|是在L1 norm下的拟合的误差,若|yi-(Axi+b)|<E,则不惩罚。
	
	#为获得稀疏解,计算超平面参数A,b不依靠所有样本数据,而是部分数据
	#不受惩罚的样本点对A,b计算没有贡献。分类问题中yi(Axi-b)<1的点,回归问题中|yi-(Axi+b)|>E的点
	#SVM结构风险最小化，在复杂度和误差取折中
	#tf.mul  tf.sub   tf.neg 已经废弃分别可用tf.multiply  tf.subtract  tf.negative替代.	
	
	线性SVM中，我们使用了一个特定的损失函数来解决软边界问题。另一种方法是解决对偶问题的优化问题,优化对偶问题，然后通过拉格朗日乘数alpha计算w,b.
	二分类到多分类策略
		one versus all 
			多个类别转化成两类来实现。在训练时，对于k个类别的样本数据，需要训练k个SVM二类分类器
			如果只有一个分类器输出正值，那么可直接判决结果为相应分类器编号，否则选取判别函数值最大所对应的类别为测试数据的类别			
			由于负类样本的数据要远远大于正类样本的数据，从而出现了样本不对称的情况
			从“一对多”的方法又衍生出基于决策树的分类
				首先SVM将所有类别分为两个类别，再将子类进一步划分为两个次级子类，如此循环下去，直到所有的节点都只包含一个单独的类别为止
																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																

		one versus one
			两个不同类别构成一个SVM子分类器，这样对于K个类别来说，共有（k*(k-1)/2）个分类器
			有向无环图（Directed Acyclic Graph）的分类方法，训练过程如“一对一”类似
				
KNN
	通过交叉验证调整超参数
	tf.nn.top_k最大的k个值
	Levenshtein距离(编辑距离).TensorFlow的Levenshtein距离函数是edit_distance()
	MNIST手写数字数据集,由数千个尺寸为28x28像素的标记图像组成.
	
-----------------
NLP
	we must find a way to convert the text into numbers
	词袋模型
		通过数据生成一个词典，将句子映射为one-hot向量
		在这个例子中，将文本大小限制为25个字。这是一个常见的做法，它限制了文本长度对预测的影响。 你可以想象，如果我们找到一个词，例如会议，这个预测文本是ham（而不是垃圾邮件），那么一个垃圾邮件可能会通过在这个词的最后出现多次出现。确定最大的句子大小的方法是看一下数据集文本长度直方图，一个好的截止可能是25个字左右。 
		
	tf-idf
		一个词在某一篇文档频繁出现且不是在很多篇文章频繁出现则词语价值最大
			tf 单词在当前文档的频率
			idf 单词在all文档的频率
		处理了单词重要性的问题，但没有解决单词排序的问题,没有考虑到句子中词序的功能
	
	use神经网络，我们可以使嵌入值成为训练过程的一部分
		Word2vec
	
	捕获单词关系方面的单词嵌入,试图理解各个词是如何相互关联的.
		加载文本，转换为小写，去除标点符号空白等，去除停用词
		生成词汇表，原始文本可以转化为词汇表的索引表示
		N=|D|,词汇表长度为N,单词设定嵌入数值向量长度为m，每一个词一个向量，因此V为N by m
		CBOW和skip-gram输出层输出最可能的w，词汇量共|D|个，所以可将其视为一个多分类问题，给定特征，从|D|个分类中选择
		多分类最常用softmax回归,分类的类别太多时，计算量很大，需要简化softmax计算，采取估算的方法得到softmax的值
		
		NEG
			负采样,Noise-Contrastive Estimation（NCE，噪声对比估计）的简化版本.
			把语料中的一个词串的中心词替换为别的词，构造语料中不存在的词串作为负样本。最大化正样本的概率，同时最小化负样本的概率。
		
		CBOW
			(context(w),w),给定context(w)下，希望增大p(w|context(w)的概率，降低p(NEG(w)|context(w))的概率.
			在看到context(w)下，希望预测到w,而不是预测到非w的NEG(w)
			算法步骤:
				若window_size=2,则一个样本对为[context(w2)=(Vw0,Vw1,Vw3,Vw4),Vw2]
				h=sum(context(w2))
				#p(w2|context(w2))=softmax(h.T*vw2)=e^h.T*vw2/all(e^h.T*vwi),all是归一化概率
				估算	softmax,使用NEG
					对于context(w2),w2正样本，通过NEG采样不是w2而是负样本.
					最大化p(w2|context(w2))*1-p(NEG(w2)|context(w2))的概率，即是w2而不是其它的概率
					加强目标，抑制其它		
		skip-gram
			(w,context(w)),给定目标词w下，增大正样本p(context(w)|w)的概率，降低p(NEG(context(w))|w)的概率
			在看到目标词，关联的上下文应该是context(w)之一，而不是除了context(w)的NEG(context(w)).
			算法步骤:
				若window_size=2,[Vw2,context(w2)=(Vw0,Vw1,Vw3,Vw4)]	#(w,context(w))
				若直接按照一个[Vw2,context(w2)=(Vw0,Vw1,Vw3,Vw4)]为一个样本整体计算则是一个多输出的softmax,拆分为[(Vw2,Vw0),(Vw2,Vw1),(Vw2,Vw3),(Vw2,Vw4)].则是将多分类拆分为了多个二分类的连乘。
				
				skip-gram生成多个样本[(Vw2,Vw0),(Vw2,Vw1),(Vw2,Vw3),(Vw2,Vw4)]
				循环列表[(Vw2,Vw0),(Vw2,Vw1),(Vw2,Vw3),(Vw2,Vw4),...]
					对于一个样本:(Vw2,Vw0)
					h=Vwi,i=0,1,3,4
					估算	softmax,使用NEG
						希望在Vw2下看到Vwi,p(Vwi|Vw2)
						因此wi是正样本，通过NEG采样不是wi是负样本.
						最大化p(wi|w2)*1-p(NEG(wi)|w2)的概率，即是wi而不是其它的概率.
						
				#依据以上描述对于每一个(w,context(w)),需要对context(w)中每一个词进行负采样NEG(wi)
				#wordvec源码不是这样编程的，它对w采样了|context(w)|次
						
				
				#分析一下差别
				NEG(w0) 不是w0的其它 NEG(w2) 不是w2的其它
				NEG(w1) 不是w1的其它 NEG(w2) 不是w2的其它
				NEG(w3) 不是w3的其它 NEG(w2) 不是w2的其它
				NEG(w4) 不是w4的其它 NEG(w2) 不是w2的其它
				
				#用CBOW考虑skip-gram
				即将CBOW的样本[context(w2)=(Vw0,Vw1,Vw3,Vw4),Vw2]拆为单个
				循环列表[(Vw0,Vw2),(Vw1,Vw2),(Vw3,Vw2),(Vw4,Vw2)]
					由于输入只有一个vwi,因此输入求和h=vwi
					h=vwi
					估算	softmax,使用NEG
						希望在Vwi下看到Vw2,p(Vw2|Vwi)
						因此Vw2是正样本，NEG(w2)是负样本
						最大化p(w2|wi)*1-p(NEG(w2)|wi)概率
						
					#每一循环采样都是NEG(w2),对w2采样了|context(w2)|次
					#由于CBOW和skip-gram的p(wi|w2)和p(w2|wi)形式是一样的都等于sigmod(h.T*wi)
					#因此skip-gram源码中的实现对w采样了|context(w)|次,相当于使用了拆分为单样本的CBOW	
					
		use word2vec的情感分析
			通过word2vec训练将词语嵌入到K维向量空间，通过对词向量计算均值处理，获得句子向量.use机器学习分类算法预测句子的情感正负.	
			word2vec在微博等短文的效果良好,短文单词少平均化处理后仍能保持相关性。
			在分析段落数据时，忽略上下文和单词顺序等信息，将会丢掉许多重要的信息。分析长篇评论采用Doc2vec更好。
							
	Doc2Vec
		COBW预测wt时仅使用了window内的[wt-1,wt-2,wt+1,wt+2],DOBW引入了wt来自于哪个doc的信息
		单词嵌入与文档嵌入结合起来有两种主要方法。
			文档嵌入向量被累加到单词嵌入向量(or均值)
			被连接到单词嵌入的结尾组成更长的向量
			
/*
语言模型（Language Model)是描述自然语言内在规律的数学模型.语言模型可分为
	文法型语言模型
		人工编制的语言学文法，文法规则来源于语言学家掌握的语言学知识和领域知识
	统计语言模型
		借助统计语言模型的概率参数，估计出自然语言句子出现的可能性，而不是简单的判断句子是否符合文法
		常用统计语言模型有N元文法模型（N-gram Model）、隐马尔科夫模型（Hidden Markov Model，HMM）、最大熵模型（Maximum Entropy Model）
	N元文法模型（N-gram Model）
		自然语言中，存在着后一个词的出现条件地依赖于前一个词的出现的现象.假设在一个语句中第N个词出现的概率，条件依赖于它前面的N-1个词，即将一个词的上下文定义为该词前面出现的N-1个词，这样的语言模型叫做N-gram模型（N元文法统计模型）。
		N=1,p(wi=w|c) = p(wi=w) = Nw/N,c是词的上下文,上下文无关，称为Unigram Model（一元文法统计模型）
		N=2,后一个词的出现条件地依赖于前一个词的出现,（称为Bigram语言模型）	
		N=3,　Trigram
*/	

/×
词向量是词的特征表示，如果做NLP，可以迁移别人训练好的主题相近的嵌入矩阵。
句子由词组成，一个词是一个向量，则一个句子是一个矩阵m个词,n长的vector.m by n,一个样本是一个二维矩阵，可视作一幅图片输入CNN中做分类
×/
-----------------
CNN	
	不同运算的神经单元，构造出不同的layer,深度学习组合多个layer，构建深层次神经网络，并提出了更快速度的新的优化算法.
	卷积layer
		tf.nn.conv2d
	池化layer
		tf.nn.max_pool 
	优化器-tf.train.*Optimizer
	
	BN layer
		使训练更稳定，应用BN有两个选择，在进入激活函数前BN，或对激活函数输出BN.
		mini batch训练时一般采用估指数加权平均追踪估算均值和方差
		在测试时继续在训练数据的指数加权上累加估算均值和方差
		
	扩展CNN网络深度的方法:we just repeat the convolution, maxpool, ReLU series until we are satisfied with the depth. 
	CNN最后一层输出一般接一个softmax函数，输出符合概率分布	
	local response normalization
		对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力

	nbyn 卷积 fbyf ,填充p,步长s。则输出矩阵维度为[(n+2p-f)/s+1]by[(n+2p-f)/s+1],非整除向下取整  
	卷积操作是一种特征提取
	ReLu操作对特征Wi*x+b，对特征i自身来讲进行了线性变换，抑制了负值。不同的Wi角度看，是一种特征选择，对优化目标重要的特征优化后权重大，次要特征权重小.
	图片具有局部相关性，池化每个局部选择一个代表，是一种信息压缩，去相关的过程.池化nH*nW*c->[(nH-f)/s+1]*[(nW-f)/s+1]*c
	
	CNN可视化
		思想:让输入的随机图片or图片向最小化loss的方向变化来实现生成图片,不同目的构造的不同loss使图片向你想要的方向变化.	
		艺术风格转换神经算法 
			使用imagenet-vgg-19作为已经训练好的CNN网络，计算loss函数
			计算了三种损失函数：原始图像损失，风格损失和总变差损失
			训练随机噪声图像，以具有风格图像的风格和原始图像的内容	
			总变差损失，为生成图片更加光滑引入总变差函数
			
			J=a*Jc(C,G)+b*Js(S,G)	内容代价+风格代价
			内容代价函数
				使用一个预训练过的CNN,选择第L层，第L层输出为aL(.)
				Jc=||aL(C)-aL(G)||^2，网络对图片第L层的响应是学习到图片的第L级别抽象的特征，以第L级别抽象的特征来度量内容相似度。
				/*
				对于图片，L=0则是度量像素级别的相似度,L太深则抽象层次太高，细节丢失严重，内容相似度度量效果差
				*/
			风格代价
				不同通道学习同一抽象层次下图片不同特征，用相关系数描述了特征A和特征B同时出现的概率
				因此不同通道构造的Gram矩阵描述了不同特征的关系，以此种关联关系来描述了风格
				以第L层不同通道的响应，构造一个通道Gram矩阵,以矩阵F范数度量风格代价
				Gram矩阵矩阵的元素,Gij是第i通道与第j通道所有对应元素乘积求和，即两个通道元素拉成向量求內积	
				
		deepdream
			选择已经训练的神经网络，将网络的某层特征附加到图片中
				选择某层的多个filter,A=均值化filter，代表被选择的特征
				原图为B,计算A和B的差别g
					g的计算
						对不同尺度的B
							对B进行平移
							loop选择子区域计算差别得到g
				若g=A-B,则B+g=B+A-B=A,因此B+=n*g,B更靠近A
				
		对象检测
			滑动窗口对象检测算法
				训练一个可以识别是否存在目标的卷积神经网络
				在测试图片上使用一个特定大小的滑动窗口，使用CNN判断滑动窗口内是否存在目标
			/x
			滑动窗口太大精度不够，太小则格子多计算量大
			x/
			YOLO
				对象检测算法评价指标
					IOU=交／并
				非最大抑制
					排除置信度<0.6边框的	
					输出最大置信度的边框
					去除和最大置信度边框IOU>0.5的边框	
-----------------
RNN
	循环神经网络中一些重要的设计模式包括以下几种:
		1. 每个时间步都有输出,并且隐藏单元之间有循环连接的循环网络.
		2. 每个时间步都产生一个输出,只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络
		3. 隐藏单元之间存在循环连接,但读取整个序列后产生单个输出的循环网络
		
	词嵌入是一种思想，不只CBOW和Skip gram,CBOW和Skip gram的对上下文建模的副产品,基于rnn的spam分类也使用了词嵌入。
	即根据目的可以训练词的嵌入表示,符合DL让数据说话的思想
	词嵌入是词的一种特征表示,可以迁移已经训练的词嵌入矩阵
	
	
	tf.nn，tf.layers， tf.contrib有很多功能是重复的，封装程度逐渐递进
		tf.nn ：提供神经网络相关操作的支持，包括卷积操作（conv）、池化操作（pooling）、归一化、loss、分类操作、embedding、RNN、Evaluation。
        tf.layers：主要提供的高层的神经网络，主要和卷积相关的，感觉是对tf.nn的进一步封装，tf.nn会更底层一些。
        tf.contrib:	由开源社区贡献，根据反馈意见改进性能，改善API友好度，API稳定后，移到TensorFlow核心模块。
    
	多层lstm莎士比亚预测。
		第一个变化，三层LSTM模型，而不是只有一个层。将上一层的输出接入到下一层的输入，堆叠产生更深的rnn网络。
		第二个变化,做字符级别的预测而不是单词,会将潜在输出大减少到只有40个字符（26个字母，10个数字，1个空白和3个特殊字符）
		单层是单词->单词的映射ss
		多层是字符序列->字符的映射，类比CNN从最低级别抽象学习，利用多层网络自动提取特征,学习知识.字符->单词->句子	
	dropout指网络中数据流转时以一定概率(keep prob)正常输出,否则置0,可有效防止过拟合。    
    	self.lstm_cell=tf.nn.rnn_cell.DropoutWrapper(self.lstm_cell, input_keep_prob=1.0,output_keep_prob=1.0, state_keep_prob=1.0,...) 
		此函数是rnn cell的dropout和tf.nn.dropout有些不同,这里对上文定义的self.lstm_cell包装了Dropout，可控制输入，输出和状态流转的dropout
		
	句子按长度分桶
		句子长短不一致，为不同长度的句子创建不同的模型可以最小化填充字符对较短句子的影响（填充太多0不仅占用内存而且弱化了句子拥有的特征作用）
		将句子按长度划分到不同桶中，同一个桶有一个最大长度，使用一个模型.
		
	注意力模型
		encoder->h->decoder |CNN->h->decoder 模型中CNN或encoder学习图像|文本后输出一个固定的向量
		注意力模型模仿人对不同位置注意力不同，不使用一个固定的h而是使用多个向量代表图片不同位置or序列不同时间
		向量加权产生不同向量hi,权值代表了hi对不同位置的注意力.			
/x
RNN是将序列转化为向量
CNN是将图片转化为向量
将跳连接引入RNN，创建高速的梯度highway，借鉴此思想产生了lstm
x/	
	双向RNN
		每一个训练序列向前和向后两个RNN都连接输出层,此结构为输出层提供了输入序列每一个点的完整的过去和未来的上下文信息，向前和向后隐含层之间没有信息流。
--------------	
生产代码将被定义为
	具有单元测试
		每个单元测试某个指标是否满足不满足assert
	分离train和eval
	高效地保存和加载数据
	创建graph session
	
-------------
分词
	机械分词
		基于词典匹配
		机械分词基于最大匹配方法 MM
		
	基于语法和规则
	基于统计的分词
		字与字共现频率反映词的可信度

	汉字约5万个，常用6000个
	计算机语言界统一了分词规范(制定标准)
	
	分词粒度
		粗粒度 浙江大学/在/杭州  主要用于NLP应用
		细粒度 浙江/大学/在/杭州 主要用于搜索引擎
	
	未登录词
		不包含在词典中，因为人在实践中不断创建新词
		九成是专用名词，如人名，译名等
		专有名词处理，命名实体识别	
			semi-CRF 算法
	中文分词ICTCLAS系统
	
	
很多传统机器学习算法被用于NLP但相互独立没有形成体系,目前形成体系的NLP流行算法思想有两大流派
	NLP 概率图模型
		朴素贝叶斯模型
		隐马尔科夫模型
		最大熵模型
		条件随机场			
	基于NN的深度学习
		word2vec
		rnn
		lstm	

还原论思想：复杂由基本简单元素组成，因此解决问题思路从定义或寻找基本元素开始，由简单到复杂.
	
语义与认知
	语义三角论
		意义、概念和客观事物是直接联系
		概念与符号是直接联系，概念通过符号表达
		符合与事物没有直接必然联系，二者是约定俗称的
	
	事物->人:形成概念，意义是认知过程
	人->符号:人通过符号表达意义和概念是编码过程

---------
指数加权平均
	u[t]=Beta*u[t-1]+(1-Beta)theta[t] 
	u[t]是指数平均，近似计算了t时刻和t时刻之前的总共1/(1-Beta)时刻的平均值
	保存1/(1-Beta)时刻的所有theta[i]值可以直接计算一个算术平均值，但指数加权方法可以近似估计不用存储theta[i]的值	

浮点数精度
	使用更少的位，计算更快更节省内存
	前向计算可以比后向梯度计算使用更少的位来加快计算
	更少的位可能避免了过拟合，效果更好
	以低精度存储参数，计算乘积时转换为更高精度计算，将结果随机四舍五入到低精度
	用定点数存储计算
	训练时以更高精度更精确，使用时压缩网络

端到端学习
	传统机器学习系统一般从原始数据到目标会有多个步骤，分解为子问题。如针对问题人工提取特征
	DL流行的端到端学习，直接使用一个NN映射Ｘ->Y
	端到端学习需要有足够的数据，在大量数据下端到端学习会取得优势,如果数据量不足，子问题分解，可能更好.
	p2p学习没有人工设计组件的干预过程，做到了just data speak
	如果简单的处理可以提高准确性，那么在p2p之前处理一个是值得的

迁移学习
	目标问题数据量少，类似的问题有大量数据
	可以使用在另一个类似问题的数据训练一个网络，然后将网络知识迁移到目标问题对网络最后的几层重新预训练和微调
	任务A迁移知识到B，能改进Ｂ的性能才是有意义的

多任务学习
	训练一个网络做四件事情，比训练四个网络性能要好，不同任务之间知识共享相互促进.
	多任务之间底层知识可以共享才适用在一起训练
	建议不同任务数据量对等,非严格约束
